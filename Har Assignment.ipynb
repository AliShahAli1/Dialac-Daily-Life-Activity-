{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Human Activity Recognitioon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sprint-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "from scipy import signal\n",
    "import matplotlib.pyplot as plt \n",
    "import math\n",
    "from sklearn import preprocessing\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import make_scorer, accuracy_score, confusion_matrix\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Visualization \n",
    "#### 1. load the  dataset_1 with each activity\n",
    "#### 2. visualize the dataset using matplot the accelerometer and gyroscope values are used for visualization with ( wrist, Chest, Hip, Anckle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#.....\n",
    "#Visualze the Accelerometer and Gyroscope data of wrist position with all categories and Partiicipants\n",
    "#....Participant--19\n",
    "#....Activities--13\n",
    "#....Sensors two (Accelerometer and Gyroscope)\n",
    "#.....\n",
    "\n",
    "def data_visulization():\n",
    "    \n",
    "    for b in range(1,20):\n",
    "        for a in range(1,14):\n",
    "            # read dataset file\n",
    "            df = pd.read_csv('dataset_'+str(b)+'.txt', sep=',', header=None)\n",
    "            if a==1:\n",
    "                label='sittinng'\n",
    "            elif a==2:\n",
    "                label='Lying'\n",
    "            elif a==3:\n",
    "                label='Standing'\n",
    "            elif a==4:\n",
    "                label='Washing dishes'\n",
    "            elif a==5:\n",
    "                label='Vacuuming'\n",
    "            elif a==6:\n",
    "                label='Sweeping'\n",
    "            elif a==7:\n",
    "                label='Walking'\n",
    "            elif a==8:\n",
    "                label='Ascending stairs'\n",
    "            elif a==9:\n",
    "                label='Descending stairs'\n",
    "            elif a==10:\n",
    "                label='Treadmill running'\n",
    "            elif a==11:\n",
    "                label='Bicycling on ergometer (50W)'\n",
    "            elif a==12:\n",
    "                label='Bicycling on ergometer (100W)'\n",
    "            else:\n",
    "                label='Rope jumping'\n",
    "            df_label = df[df[24] == a].values\n",
    "            #Visualisation with Acceleroometer.....\n",
    "            print(\"Participant_: \",b)\n",
    "            plt.plot(df_label[:,0:3])\n",
    "            print(label+\":  Accelerometer \")\n",
    "            plt.show()\n",
    "            #Visualiisation  with Gyroscope\n",
    "            print(\"Participant_: \",b)\n",
    "            print(label+\":  Gyroscope\")\n",
    "            plt.plot(df_label[:,5:6])\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Note:\n",
    " we can use the hip, chest and ankle  data by changiing the df_label index\n",
    "#..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calling out the visualization method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_visulization()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sprint-2\n",
    "1. Noise Removal \n",
    "2. Visualization after applying low pass filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#'''\n",
    "#For raw sensor data, it usually contains noise that arises from different sources, such as sensor mis-\n",
    "#calibration, sensor errors, errors in sensor placement, or noisy environments. We could apply filter to remove noise of sensor data\n",
    "#to smooth data. In this example code, Butterworth low-pass filter is applied. \n",
    "#'''\n",
    "def noise_removing():\n",
    "    for e in range(1,20):\n",
    "        for a in range(1,14):\n",
    "            df = pd.read_csv('dataset_'+str(e)+'.txt', sep=',', header=None)\n",
    "            if a==1:\n",
    "                label='sittinng'\n",
    "            elif a==2:\n",
    "                label='Lying'\n",
    "            elif a==3:\n",
    "                label='Standing'\n",
    "            elif a==4:\n",
    "                label='Washing dishes'\n",
    "            elif a==5:\n",
    "                label='Vacuuming'\n",
    "            elif a==6:\n",
    "                label='Sweeping'\n",
    "            elif a==7:\n",
    "                label='Walking'\n",
    "            elif a==8:\n",
    "                label='Ascending stairs'\n",
    "            elif a==9:\n",
    "                label='Descending stairs'\n",
    "            elif a==10:\n",
    "                label='Treadmill running'\n",
    "            elif a==11:\n",
    "                label='Bicycling on ergometer (50W)'\n",
    "            elif a==12:\n",
    "                label='Bicycling on ergometer (100W)'\n",
    "            else:\n",
    "                label='Rope jumping'\n",
    "        # Butterworth low-pass filter. You could try different parameters and other filters. \n",
    "            b, d = signal.butter(4, 0.04, 'low', analog=False)\n",
    "            df_label = df[df[24] == a].values\n",
    "            for i in range(3):\n",
    "                df_label[:,i] = signal.lfilter(b, d, df_label[:, i])\n",
    "            plt.plot(df_label[:, 0:3])\n",
    "            print(\"Participant_: \",b)\n",
    "            print(\"Accelerometer Data\",label)\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calling out the noise method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# noise_removing()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sprint- 3\n",
    "1. Feauture Engineering\n",
    "2. Noise Removal\n",
    "3. Splitting Training and Testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#'''\n",
    "#To build a human activity recognition system, we need to extract features from raw data and create feature dataset for training \n",
    "#machine learning models.\n",
    "\n",
    "#Please create new functions to implement your own feature engineering. The function should output training and testing dataset.\n",
    "#'''\n",
    "def feature_engineering_Noise():\n",
    "    training = np.empty(shape=(0, 49))\n",
    "    testing = np.empty(shape=(0, 49))\n",
    "    # deal with each dataset file\n",
    "    for i in range(19):\n",
    "        df = pd.read_csv('dataset_' + str(i + 1) + '.txt', sep=',', header=None)\n",
    "        print('deal with dataset ' + str(i + 1))\n",
    "        for c in range(1, 14):\n",
    "            activity_data = df[df[24] == c].values\n",
    "            b, a = signal.butter(4, 0.04, 'low', analog=False)\n",
    "            for j in range(24):\n",
    "                activity_data[:, j] = signal.lfilter(b, a, activity_data[:, j])\n",
    "            \n",
    "            datat_len = len(activity_data)\n",
    "#             print(datat_len)\n",
    "            training_len = math.floor(datat_len * 0.8)\n",
    "            training_data = activity_data[:training_len, :]\n",
    "            testing_data = activity_data[training_len:, :]\n",
    "\n",
    "            # data segementation: for time series data, we need to segment the whole time series, and then extract features from each period of time\n",
    "            # to represent the raw data. In this example code, we define each period of time contains 1000 data points. Each period of time contains \n",
    "            # different data points. You may consider overlap segmentation, which means consecutive two segmentation share a part of data points, to \n",
    "            # get more feature samples.\n",
    "            training_sample_number = training_len // 1000 + 1\n",
    "            testing_sample_number = (datat_len - training_len) // 1000 + 1\n",
    "#             print(\"testingg sample number: \",testing_sample_number)\n",
    "#             print(\"trainingg:\",training_sample_number)\n",
    "\n",
    "\n",
    "#Features Generation or extraction method.......\n",
    "            \n",
    "            for s in range(training_sample_number):\n",
    "                if s < training_sample_number - 1:\n",
    "                    sample_data = training_data[1000*s:1000*(s + 1), :]\n",
    "                else:\n",
    "                    sample_data = training_data[1000*s:, :]\n",
    "                # in this example code, only three accelerometer data in wrist sensor is used to extract three simple features: min, max, and mean value in\n",
    "                # a period of time. Finally we get 9 features and 1 label to construct feature dataset. You may consider all sensors' data and extract more\n",
    "#                 print(\"samplle_data: \",sample_data)\n",
    "                feature_sample = []\n",
    "                for i in range(12):\n",
    "                    feature_sample.append(np.min(sample_data[:, i]))\n",
    "                    feature_sample.append(np.max(sample_data[:, i]))\n",
    "                    feature_sample.append(np.mean(sample_data[:, i])) \n",
    "                    feature_sample.append(np.std(sample_data[:, i])) \n",
    "                feature_sample.append(sample_data[0, -1])\n",
    "                feature_sample = np.array([feature_sample])\n",
    "#                 print(\"featuure_samplle:\",feature_sample.shape)\n",
    "#                 print(\"training_data:\",training.shape)\n",
    "                \n",
    "#                 print(feature_sample.shape)\n",
    "                training = np.concatenate((training, feature_sample), axis=0)\n",
    "            \n",
    "            for s in range(testing_sample_number):\n",
    "                if s < training_sample_number - 1:\n",
    "                    sample_data = testing_data[1000*s:1000*(s + 1), :]\n",
    "                else:\n",
    "                    sample_data = testing_data[1000*s:, :]\n",
    "\n",
    "                feature_sample = []\n",
    "                for i in range(12):\n",
    "                    feature_sample.append(np.min(sample_data[:, i]))\n",
    "                    feature_sample.append(np.max(sample_data[:, i]))\n",
    "                    feature_sample.append(np.mean(sample_data[:, i]))\n",
    "                    feature_sample.append(np.std(sample_data[:, i]))\n",
    "                \n",
    "                feature_sample.append(sample_data[0, -1])\n",
    "                feature_sample = np.array([feature_sample])\n",
    "                testing = np.concatenate((testing, feature_sample), axis=0)\n",
    "                \n",
    "\n",
    "    df_training = pd.DataFrame(training)\n",
    "    df_testing = pd.DataFrame(testing)\n",
    "    df_training.to_csv('training_data_2.csv', index=None, header=None)\n",
    "    df_testing.to_csv('testing_data_2.csv', index=None, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deal with dataset 1\n",
      "deal with dataset 2\n",
      "deal with dataset 3\n",
      "deal with dataset 4\n",
      "deal with dataset 5\n",
      "deal with dataset 6\n",
      "deal with dataset 7\n",
      "deal with dataset 8\n",
      "deal with dataset 9\n",
      "deal with dataset 10\n",
      "deal with dataset 11\n",
      "deal with dataset 12\n",
      "deal with dataset 13\n",
      "deal with dataset 14\n",
      "deal with dataset 15\n",
      "deal with dataset 16\n",
      "deal with dataset 17\n",
      "deal with dataset 18\n",
      "deal with dataset 19\n"
     ]
    }
   ],
   "source": [
    "feature_engineering_Noise()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning Modeling/ Classifiers\n",
    "1. KNN (k nearest neighbor)\n",
    "2. SVM (Support Vector Machine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9069097888675623\n",
      "Confusion Matrix\n",
      "\n",
      "[[ 56   0   0   1   0   0   0   0   0   0   0   0   0]\n",
      " [  0  57   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  4   0  50   1   0   0   1   0   0   0   1   0   0]\n",
      " [  0   0   0  97   0   0   1   0   0   0   1   0   0]\n",
      " [  0   0   1   1  51   3   0   0   0   0   0   1   0]\n",
      " [  0   0   0   1  14  67   2   1   0   0   0   0   0]\n",
      " [  0   0   0   1   3   1 209   4   1   0   0   0   0]\n",
      " [  0   0   1   0   0   0   7  31   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   2   1  35   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0  96   0   0   0]\n",
      " [  0   0   0   1   2   4   0   0   0   0  75  18   0]\n",
      " [  0   0   0   1   0   0   0   0   0   0  16  83   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0  38]]\n",
      "best clf: SVC(C=100, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma=0.01, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n",
      "Accuracy:  0.9126679462571977\n",
      "Confusion Matrix\n",
      "\n",
      "[[ 55   0   0   1   0   1   0   0   0   0   0   0   0]\n",
      " [  0  57   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  4   0  52   1   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   1  96   0   0   0   0   0   0   2   0   0]\n",
      " [  0   0   2   2  44   8   0   0   0   0   0   1   0]\n",
      " [  0   0   0   3   8  74   0   0   0   0   0   0   0]\n",
      " [  0   0   1   1   3   5 206   2   1   0   0   0   0]\n",
      " [  1   0   0   0   0   1   4  33   0   0   0   0   0]\n",
      " [  0   0   0   0   0   1   1   0  36   0   0   0   0]\n",
      " [  0   0   0   0   0   1   0   0   0  95   0   0   0]\n",
      " [  1   0   0   1   0   3   0   0   0   0  82  13   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0  17  83   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0  38]]\n"
     ]
    }
   ],
   "source": [
    "#'''\n",
    "#Please create new functions to fit your features and try other models.\n",
    "#'''\n",
    "def model_training_and_evaluation():\n",
    "    \n",
    "    df_training = pd.read_csv('training_data_2.csv', header=None)\n",
    "    df_testing = pd.read_csv('testing_data_2.csv', header=None)\n",
    "\n",
    "    y_train = df_training[48].values\n",
    "    # Labels should start from 0 in sklearn\n",
    "    y_train = y_train - 1\n",
    "    df_training = df_training.drop([48], axis=1)\n",
    "    X_train = df_training.values\n",
    "\n",
    "    y_test = df_testing[48].values\n",
    "    y_test = y_test - 1\n",
    "    df_testing = df_testing.drop([48], axis=1)\n",
    "    X_test = df_testing.values\n",
    "    #............................Normalization........................\n",
    "    \n",
    "    \n",
    "    # Feature normalization for improving the performance of machine learning models. In this example code, \n",
    "    # StandardScaler is used to scale original feature to be centered around zero. You could try other normalization methods.\n",
    "    scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "    X_train = scaler.transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    \n",
    "    #...........................KNN..................................\n",
    "    \n",
    "    # Build KNN classifier, in this example code\n",
    "    knn = KNeighborsClassifier(n_neighbors=3)\n",
    "    knn.fit(X_train, y_train)\n",
    "\n",
    "    # Evaluation. when we train a machine learning model on training set, we should evaluate its performance on testing set.\n",
    "    # We could evaluate the model by different metrics. Firstly, we could calculate the classification accuracy. In this example\n",
    "    # code, when n_neighbors is set to 4, the accuracy achieves 0.757.\n",
    "    y_pred = knn.predict(X_test)\n",
    "    print('Accuracy: ', accuracy_score(y_test, y_pred))\n",
    "    # We could use confusion matrix to view the classification for each activity.\n",
    "    print(\"Confusion Matrix\\n\")\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    \n",
    "    #.............................Support Vector Machine...............................\n",
    "    \n",
    "    \n",
    "    # Another machine learning model: svm. In this example code, we use gridsearch to find the optimial classifier\n",
    "    # It will take a long time to find the optimal classifier.\n",
    "    # the accuracy for SVM classifier with default parameters is 0.71, \n",
    "    # which is worse than KNN. The reason may be parameters of svm classifier are not optimal.  \n",
    "    # Another reason may be we only use 9 features and they are not enough to build a good svm classifier. \n",
    "    tuned_parameters = [{'kernel': ['rbf'], 'gamma': [1e-1,1e-2, 1e-3, 1e-4],\n",
    "                     'C': [1e-3, 1e-2, 1e-1, 1, 10, 100, 100]},\n",
    "                    {'kernel': ['linear'], 'C': [1e-3, 1e-2, 1e-1, 1, 10, 100]}]\n",
    "    acc_scorer = make_scorer(accuracy_score)\n",
    "    grid_obj  = GridSearchCV(SVC(), tuned_parameters, cv=10, scoring=acc_scorer)\n",
    "    grid_obj  = grid_obj .fit(X_train, y_train)\n",
    "    clf = grid_obj.best_estimator_\n",
    "    print('best clf:', clf)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print('Accuracy: ', accuracy_score(y_test, y_pred))\n",
    "    print(\"Confusion Matrix\\n\")\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# print(\"# Tuning hyper-parameters for %s\" % score)\n",
    "# print()\n",
    "# clf = GridSearchCV(SVC(), tuned_parameters, cv=10,\n",
    "#                    scoring=score)\n",
    "# clf.fit(x_train, y_train)\n",
    "\n",
    "#........................................Main Method.................\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    # data_visulization()\n",
    "    # noise_removing()\n",
    "    # feature_engineering_example()\n",
    "    model_training_and_evaluation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
